{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9a3a64",
   "metadata": {},
   "source": [
    "# Notebook 1 - Use SageMaker to develop a model\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Source libraries](#Source-libraries)\n",
    "1. [Task 1: Upload the dataset to Amazon S3](#Task-1:-Upload-the-dataset-to-Amazon-S3)\n",
    "1. [Task 2: Download the dataset to the notebook instance](#Task-2:-Download-the-dataset-to-the-notebook-instance)\n",
    "1. [Optional Task: Run Exploratory Data Analysis (EDA) on your dataset](#Optional-Task:-Run-Exploratory-Data-Analysis-(EDA)-on-your-dataset)\n",
    "1. [Task 3: Data Preprocessing](#Task-3:-Data-Preprocessing)\n",
    "1. [Task 4: Model Training and Evaluation](#Task-4:-Model-Training-and-Evaluation)\n",
    "1. [Task 5: Inference and Model Evaluation](#Task-5:-Inference-and-Model-Evaluation)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4548d74b",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the use of Amazon SageMaker XGBoost to process data, train a model using a Jupyter notebook. You can run this solution using an [Amazon SageMaker Notebook Instance](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html) or using [Amazon Sagemaker Studio Notebooks](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html).\n",
    "\n",
    "We use the [Predictive Maintenance Dataset](https://static.us-east-1.prod.workshops.aws/public/6f2f7cb1-bfda-4b34-ae39-928502784393/static/datasets/maintenance_dataset.csv), originally from the [UCI data repository](http://archive.ics.uci.edu/ml). More details about the original dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/AI4I+2020+Predictive+Maintenance+Dataset).\n",
    "\n",
    "---\n",
    "\n",
    "## Source libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae78222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 1 \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e118887",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: Upload the dataset to Amazon S3\n",
    "\n",
    "Before running the cells below, \n",
    "1. Create an Amazon S3 bucket. Note that the bucket name must be globally unique.\n",
    "1. Download the [Predictive Maintenance Dataset](https://static.us-east-1.prod.workshops.aws/public/6f2f7cb1-bfda-4b34-ae39-928502784393/static/datasets/maintenance_dataset.csv)\n",
    "1. Upload the file to your created bucket\n",
    "1. Put the name of your bucket and file name into S3_BUCKET and KEY respectively in `cell 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3aa91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2\n",
    "\n",
    "# Provide information where the training and validation data will be uploaded to \n",
    "S3_BUCKET = 'YOUR_S3_BUCKET' # YOUR_S3_BUCKET\n",
    "KEY = \"YOUR_OBJECT_ON_S3\" # YOUR_OBJECT_ON_S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b8ddd",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2: Download the dataset to the notebook instance\n",
    "\n",
    "Download and read the file from Amazon S3 and take a look at the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b37b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 3 \n",
    "\n",
    "print(f\"Downloading data from bucket: {S3_BUCKET}, key: {KEY}\")\n",
    "fn = \"maintenance_dataset.csv\"\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3.Bucket(S3_BUCKET).download_file(KEY, fn)\n",
    "\n",
    "print(\"Reading downloaded data.\")\n",
    "df = pd.read_csv(fn)\n",
    "os.unlink(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd1c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 4\n",
    "\n",
    "df.shape # Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188847a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 5\n",
    "\n",
    "df.head() # Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 6\n",
    "\n",
    "df.describe() # Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5539c3",
   "metadata": {},
   "source": [
    "---\n",
    "## Optional Task: Run Exploratory Data Analysis (EDA) on your dataset\n",
    "\n",
    "Run Exploratory Data Analysis (EDA) on your dataset to dive deeper into the data... We will skip this for the Immersion Day and leave this up to participants as call to action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120bdc33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 7\n",
    "\n",
    "df.groupby(\"Failure Type\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f65c441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 8\n",
    "\n",
    "df.groupby(\"Type\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533f6978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 9\n",
    "\n",
    "df.groupby(\"Product ID\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a6058b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cell 10\n",
    "\n",
    "df.groupby(\"UDI\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe1ac55",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3: Data Preprocessing \n",
    "We will split the data into train and test sets and continue preprocess using the `feature_columns_names` and `label_column` mentioned in `cell 11`. To preprocess the data\n",
    "1. Create a `train_test_split` using sklearn `cell 12`. This code will be part of the `preprocessing`. This means this code will later be re-used in our processing container.\n",
    "1. Run the `cell 16` to preprocessor that scales your numerical features and encodes your categorical features.\n",
    "1. Run the `cell 17 encode your label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc3c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 11\n",
    "\n",
    "feature_columns_names = [\n",
    "    'UDI',\n",
    "    'Product ID',\n",
    "    'Type',\n",
    "    'Air temperature [K]',\n",
    "    'Process temperature [K]',\n",
    "    'Rotational speed [rpm]',\n",
    "    'Torque [Nm]',\n",
    "    'Tool wear [min]']\n",
    "label_column = 'Failure Type'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf89e7",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c6b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 12\n",
    "\n",
    "# Your split here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6997d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "Possible solution:\n",
    "\n",
    "<code>\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        df[feature_columns_names],\n",
    "        df[label_column],\n",
    "        random_state=42,\n",
    "        train_size=0.8,\n",
    "        shuffle=True,\n",
    "        stratify=df[label_column])\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8111c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 13 \n",
    "\n",
    "# Putting the data together again so that data can be saved to S3 or any other place\n",
    "train = pd.concat(objs=[y_train, X_train], axis=1)\n",
    "validation = pd.concat(objs=[y_val, X_val], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fcfd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 14\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b868661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 15\n",
    "\n",
    "validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad164a2e",
   "metadata": {},
   "source": [
    "### Scaling and Encoding the Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88300377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 16\n",
    "\n",
    "# Scaling the numerical features\n",
    "numeric_features = [\n",
    "    'Air temperature [K]',\n",
    "    'Process temperature [K]',\n",
    "    'Rotational speed [rpm]',\n",
    "    'Torque [Nm]',\n",
    "    'Tool wear [min]']\n",
    "\n",
    "# TODO get rid of Pipeline \n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Encoding the categorical \n",
    "categorical_features = ['Type']\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "\n",
    "# Combining both transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03385701",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 17\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_val = le.transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dff547",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 4: Model Training and Evaluation\n",
    "\n",
    "1. Install XGBoost `cell 18`\n",
    "1. Set the hyper parameters and define the estimator `cell 19`\n",
    "1. Train the model `cell 20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7592595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 18\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66435742",
   "metadata": {},
   "source": [
    "### Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afbc8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 19\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1)\n",
    "\n",
    "model = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", clf)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee9991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 20\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0fbcd0",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 5: Inference and Model Evaluation\n",
    "1. Generate the prediction and evaluate the model performance `cell 21`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 21\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "y_hat = model.predict(X_train)\n",
    "\n",
    "print(\"In Sample\")\n",
    "print(classification_report(y_train, y_hat, zero_division=1))\n",
    "print(confusion_matrix(y_train, y_hat))\n",
    "print(\"Out of Sample\")\n",
    "print(classification_report(y_val, y_pred, zero_division=1))\n",
    "print(confusion_matrix(y_val, y_pred), \"\\n\")\n",
    "\n",
    "print(f\"train-recall:{recall_score(y_train, y_hat, average='macro', zero_division=True)};\")\n",
    "print(f\"validation-recall:{recall_score(y_val, y_pred, average='macro', zero_division=True)};\")\n",
    "print(f\"train-precision:{precision_score(y_train, y_hat, average='macro', zero_division=True)};\")\n",
    "print(f\"validation-precision:{precision_score(y_val, y_pred, average='macro', zero_division=True)};\")\n",
    "print(f\"train-f1:{f1_score(y_train, y_hat, average='macro', zero_division=True)};\")\n",
    "print(f\"validation-f1:{f1_score(y_val, y_pred, average='macro', zero_division=True)};\")\n",
    "\n",
    "# Optional - retrain the model on all data\n",
    "# Leave this section commented for now:\n",
    "\n",
    "\"\"\"\n",
    "X = pd.concat(objs=[X_train, X_val], axis=0)\n",
    "y = pd.concat(objs=[pd.DataFrame(y_train), pd.DataFrame(y_val)], axis=0)\n",
    "\n",
    "model = model.fit(X, y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e1c69f",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations! \n",
    "You have successfully trained your first model on an Amazon SageMaker Notebook instance!\n",
    "\n",
    "(Optional) You can now, go over [Task 4](#Task-4:-Model-Training-and-Evaluation) and [Task 5](#Task-5:-Inference-and-Model-Evaluation)\n",
    "to:\n",
    "1. Play with the hyperparameters to see if you can find a (slightly) better model \n",
    "1. Take 15 minutes to play with the algorithm - or develop your own using other RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde7525",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
